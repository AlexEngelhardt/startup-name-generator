{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- The wordlist.txt is still mandatory now, to determine `ix_to_char`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, SimpleRNN, GRU, TimeDistributed\n",
    "from keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oddish\\n', 'geodude\\n', 'pangoro\\n', 'mudbray\\n', 'giratina\\n']\n"
     ]
    }
   ],
   "source": [
    "from utils import text_to_words\n",
    "\n",
    "# Generate a list of words (including newline)\n",
    "corpus = \"pokemon\"\n",
    "textfile = \"wordlists/\" + corpus + \".txt\"\n",
    "words = text_to_words(textfile)\n",
    "\n",
    "print(words[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 words\n",
      "\n",
      "vocabulary of 27 characters, including the \\n:\n",
      "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      "First two sample words:\n",
      "['oddish\\n', 'geodude\\n']\n"
     ]
    }
   ],
   "source": [
    "# Generate the set of unique characters (including newline)\n",
    "# https://stackoverflow.com/questions/952914/making-a-flat-list-out-of-list-of-lists-in-python\n",
    "chars = sorted(list(set([char for word in words for char in word])))\n",
    "\n",
    "VOCAB_SIZE = len(chars)\n",
    "N_WORDS = len(words)\n",
    "MAX_WORD_LEN = 12  # maximum company name length\n",
    "\n",
    "\n",
    "print(N_WORDS, \"words\\n\")\n",
    "print(\"vocabulary of\", len(chars), \"characters, including the \\\\n:\")\n",
    "print(chars)\n",
    "print(\"\\nFirst two sample words:\")\n",
    "print(words[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
    "char_to_ix = {char:ix for ix, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((N_WORDS, MAX_WORD_LEN, VOCAB_SIZE))\n",
    "Y = np.zeros((N_WORDS, MAX_WORD_LEN, VOCAB_SIZE))\n",
    "\n",
    "for word_i in range(N_WORDS):\n",
    "    word = words[word_i]\n",
    "    chars = list(word)\n",
    "    word_len = len(word)\n",
    "    \n",
    "    for char_j in range(min(len(word), MAX_WORD_LEN)):\n",
    "        char = chars[char_j]\n",
    "        char_ix = char_to_ix[char]\n",
    "        X[word_i, char_j, char_ix] = 1\n",
    "        if char_j > 0:\n",
    "            Y[word_i, char_j - 1, char_ix] = 1  # the 'next char' at time point char_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_NUM = 2\n",
    "HIDDEN_DIM = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, None, 50)          15600     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, None, 50)          20200     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, None, 27)          1377      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, None, 27)          0         \n",
      "=================================================================\n",
      "Total params: 37,177\n",
      "Trainable params: 37,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_scale(probs, temperature = 1.0):\n",
    "    # a low temperature (< 1 and approaching 0) results in the char sampling approaching the argmax.\n",
    "    # a high temperature (> 1, approaching infinity) results in sampling from a uniform distribution)\n",
    "    probs = np.exp(np.log(probs) / temperature)\n",
    "    probs = probs / np.sum(probs)\n",
    "    return probs\n",
    "    \n",
    "    \n",
    "def generate_word(model, temperature = 1.0, min_word_length = 4):\n",
    "    X = np.zeros((1, MAX_WORD_LEN, VOCAB_SIZE))\n",
    "    \n",
    "    # sample the first character\n",
    "    initial_char_distribution = temp_scale(model.predict(X[:, 0:1, :]).flatten(), temperature)\n",
    "    \n",
    "    ix = 0\n",
    "    while ix == 0:  # make sure the initial character is not a newline (i.e. index 0)\n",
    "        ix = int(np.random.choice(VOCAB_SIZE, size = 1, p = initial_char_distribution))\n",
    "    \n",
    "    X[0, 0, ix] = 1\n",
    "    generated_word = [ix_to_char[ix].upper()]  # start with first character, then later successively append chars\n",
    "    \n",
    "    # sample all remaining characters\n",
    "    for i in range(1, MAX_WORD_LEN):\n",
    "        next_char_distribution = temp_scale(model.predict(X[:, 0:i, :])[:, i-1, :].flatten(), temperature)\n",
    "\n",
    "        \n",
    "        ix_choice = np.random.choice(VOCAB_SIZE, size = 1, p = next_char_distribution)\n",
    "        # ix_choice = np.argmax(next_char_distribution)  # <- corresponds to sampling with a very low temperature\n",
    "        ctr = 0\n",
    "        while ix_choice == 0 and i < min_word_length:\n",
    "            ctr += 1\n",
    "            # sample again if you picked the end-of-word token too early\n",
    "            ix_choice = np.random.choice(VOCAB_SIZE, size = 1, p = next_char_distribution)\n",
    "            if ctr > 1000:\n",
    "                print(\"caught in a near-infinite loop. You might have picked too low a temperature and the sampler just keeps sampling \\\\n's\")\n",
    "                break\n",
    "            \n",
    "        \n",
    "        next_ix = int(ix_choice)\n",
    "        X[0, i, next_ix] = 1\n",
    "        if next_ix == 0:\n",
    "            break\n",
    "        generated_word.append(ix_to_char[next_ix])\n",
    "    \n",
    "    return ('').join(generated_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"epoch \" + str(epoch) + \": \" + generate_word(model, temperature = 1.0, min_word_length = 4))\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end = on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: Htaz\n",
      "epoch 50: Eklapbk\n",
      "epoch 100: Orimlaxasp\n",
      "epoch 150: Winblis\n",
      "epoch 200: Awostas\n",
      "epoch 250: Miscanel\n",
      "epoch 300: Oleettalllol\n",
      "epoch 350: Ragty\n",
      "epoch 400: Manblate\n",
      "epoch 450: Unna\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 500\n",
    "BATCH_SIZE = 64  # or: N_WORDS\n",
    "\n",
    "model.fit(X, Y, batch_size = BATCH_SIZE, verbose = 0, epochs = NUM_EPOCHS, callbacks = [print_callback])\n",
    "\n",
    "# save the model, but only if the h5 file doesn't exist yet:\n",
    "model_filename = \"models/\" + corpus + \"_\" + str(NUM_EPOCHS) + 'epochs.h5'\n",
    "if not os.path.isfile(model_filename):\n",
    "    model.save(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load one of these models if you have trained them before and want to skip re-training\n",
    "from keras.models import load_model\n",
    "\n",
    "# model = load_model(\"models/behemoth_500epochs.h5\")\n",
    "model = load_model(\"models/pokemon_500epochs.h5\")\n",
    "# model = load_model(\"models/german_500epochs.h5\")\n",
    "# model = load_model(\"models/english_500epochs.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orbange\n",
      "Omastar\n",
      "Iniles\n",
      "Avilrash\n",
      "Heritom\n",
      "Nudilor\n",
      "Himomlow\n",
      "Ilgathin\n",
      "Izorua\n",
      "Oursla\n",
      "Regigicash\n",
      "Ordariy\n",
      "Osherdee\n",
      "Origin\n",
      "Ervesacod\n",
      "Alumanol\n",
      "Fryverllas\n",
      "Apisadon\n",
      "Oesseacule\n",
      "Owsing\n",
      "Oyslee\n",
      "Umcoot\n",
      "Izabt\n",
      "Edeveiv\n",
      "Aritoch\n",
      "Encevezezza\n",
      "Edeegtra\n",
      "Edecnukr\n",
      "Odale\n",
      "Hown\n"
     ]
    }
   ],
   "source": [
    "# Print a few words with the final model:\n",
    "\n",
    "for _ in range(30):\n",
    "    print(generate_word(model, temperature = 1, min_word_length = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
